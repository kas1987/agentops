---
id: v3-roadmap-plan-L1
type: plan
created_at: 2026-02-15
source: ag-nzua.6
category: strategy
confidence: high
---

# v3 Roadmap: Maturity → Stability → Expansion

## Executive Summary

v2.x was the arc of **discovery through stability**. Seven releases across 7 days compressed the classical product maturity curve: Build (v2.0) → Harden (v2.1-v2.2) → Self-Improve (v2.5) → Explain (v2.7) → Constrain (v2.8). The system achieved escape velocity: it now diagnoses its own weaknesses and prescribes fixes faster than human direction.

v3 enters a new phase: **stability consolidation and scaled expansion**. The core platform is proven. Three new maturity arcs await: external consumer hardening, multi-vendor platform expansion, and autonomous system scaling.

---

## v2.x Arc Summary

### What v2.x Accomplished

| Phase | Releases | Focus | Outcome |
|-------|----------|-------|---------|
| **Build** | v2.0.0 | Core RPI lifecycle | Goal-to-code pipeline stabilized |
| **Harden** | v2.1.0, v2.2.0 | Public-facing polish, release gates, pre-mortem enforcement | Skill validation >90%, pre-mortem mandatory at v2.7.1 |
| **Self-Improve** | v2.5.0, v2.6.0 | `/evolve` + GOALS.yaml fitness scoring | System detects and fixes its own gaps in 3-day cycles |
| **Explain** | v2.7.0 | README rewrite, 4-pillar framing, deliberation consensus | Product identity crystallized; 44 fitness goals captured |
| **Constrain** | v2.8.0 | `ao rpi phased`, context windowing, incident runbook | Escape velocity achieved: system runs autonomously |

### Key Learnings from v2.x

**Process Learnings:**
1. **Product maturity follows a predictable arc** — Ship → Harden → Self-Improve → Explain → Constrain. Future majors should budget 2-3 releases per phase.
2. **The "rush .0, patch .1" pattern is structural** — Every v2.0.x release had a .1 within 24h. Pre-release completeness checklist prevents this: all features wired? All tests present? All gates codified?
3. **README rewrites signal identity confusion** — v2 had 4 rewrites; identity crystallized only after deliberation consensus on 4 pillars. Write functional README early, defer narrative README until identity is clear.
4. **Pre-mortem is the only process that earned enforcement** — 7+ consecutive epics showed zero false positives. Graduated from suggestion → nudge → mandatory at v2.7.1. Process should promote based on empirical success, not policy.
5. **Self-referential improvement compounds faster than manual work** — `/evolve` + fitness scoring produced more targeted fixes in 3 days (v2.5→v2.8) than manual direction (v2.0→v2.4). Measurement infrastructure is the killer feature.

**Architecture Learnings:**
1. **Context window limits are architectural, not operational** — Designed for single-session at v2.0; by v2.8 needed `ao rpi phased` for multi-session. Budget context consumption at design time; >60% = design for multi-session from start.
2. **Empirical testing of external CLIs prevents hallucination cascades** — Wave 0 testing of Codex flags prevented 40%+ scope waste. For any external integration: run `--help`, test flag combos, document in memory before writing skills.
3. **Append-only quality gates scale safely** — GOALS.yaml expanded 22→42 goals across 3 evolve cycles with zero regressions. Goal-driven design is safe for incremental ratcheting.

---

## v3 Vision: Three Maturity Arcs

v3 is **not one release**, but a strategic roadmap across **3 distinct maturity arcs**, each requiring 3-5 releases:

### Arc 1: Consumer Hardening (v3.0.0 - v3.2.0)
**Focus:** External user readiness and runtime diversity
**Duration:** 3-4 weeks
**Success Criteria:** ≥3 external teams shipping to production with AgentOps; zero critical incidents in first 2 weeks

**Themes:**
1. **Multi-runtime certification** — Proven execution on Claude, Codex, Cursor, Open Code. Today: Claude primary, Codex secondary. Add: structured fallback matrix, runtime-native test suite, vendor lock-in documentation.
2. **Observability & incident response** — Production users need to see what's happening. Metrics dashboard (flywheel velocity, goal fitness, phase timing), structured logging (all hook events), incident runbook with recovery procedures. Today: scattered `.agents/` state. Add: unified observability layer.
3. **Consumer onboarding path** — Reduce time-to-first-success. Today: 5-step install + `/quickstart`. Add: interactive welcome wizard, guided-mode `/rpi` (hand-holds per phase), "copy-paste to production" reference configs.
4. **Backwards compatibility guarantee** — Document what won't change (CLI contract, GOALS.yaml format, hook event types). Add: semver policy, deprecation lane, migration guides for major versions.

### Arc 2: Platform Expansion (v3.3.0 - v3.5.0)
**Focus:** Multi-model consensus, non-Claude vendors, and cross-platform orchestration
**Duration:** 4-5 weeks
**Success Criteria:** `/council` runs with Claude + Codex + 1 additional LLM; ≥2 new vendor runtimes certified

**Themes:**
1. **Cross-vendor consensus protocols** — `/council` today is Claude-centric. Design for model diversity: cached disagreements, confidence weighting, vendor-neutral judge language (not "Claude thinks X, Codex thinks Y"). Requires studying how judges disagree when vendors differ.
2. **Agent team orchestration standardization** — v2 shipped native teams on Claude. Document how swarm/crank/council map to teams vs subagents. Add: Olympus bridge (v2.0 shipped `ao inject`, now finish the handoff contract). Codex sub-agents as first non-native-team path.
3. **Skill portability certification** — Today: skills work on both Claude and Codex. Add: formal test matrix (each skill on each vendor), portability guidelines, vendor-specific skill sections in SKILL.md. Validate that no skill locks to one vendor.
4. **Flywheel bridges to external systems** — Knowledge flywheel is internal to AgentOps. Connect learnings to: Beads (GitHub issues), Olympus (constraints), external knowledge bases. Add: learned-patterns export format, external knowledge injection.

### Arc 3: Autonomous Scaling (v3.6.0 - v3.8.0)
**Focus:** Multi-session autonomy, resource optimization, and learned system behavior
**Duration:** 4-5 weeks
**Success Criteria:** `/evolve` runs unattended for 7+ days; auto-fixes match manual fixes >80% of the time; system learns new skills

**Themes:**
1. **Multi-session context bridges** — Today: `/rpi phased` runs each phase in fresh session. Add: learned state persistence across sessions (not just files). Track "what worked before" and apply heuristics. Requires persistent embedding DB and retrieval at session start.
2. **Resource-aware execution** — System knows its own limits: token budget, wall-clock time, model availability. Adjust strategy: long-running phases when time-rich, short-running when token-constrained. Add: cost calculation, resource reservation, backpressure handling.
3. **Learned workflow evolution** — System patterns its own successful executions and teaches new variations. If `/crank` always adds error handling in Wave 2, system learns this as a template. Add: pattern mining from successful runs, learned-workflow registration, skill-generation from templates.
4. **Cross-project knowledge transfer** — Learnings are project-scoped today. Add: cross-project knowledge tags (e.g. "works in Node projects", "works with Postgres"). Retrieval scores based on project similarity. Enables cold-start for new projects.

---

## Release Cadence & Dependencies

### v3.0: Consumer Hardening I — Multi-Runtime Certification
**Target:** 2 weeks
**Scope:** ~12 issues
- Runtime certification suite (Claude, Codex, Cursor, Open Code)
- Observability layer (metrics, structured logging, dashboard)
- Consumer onboarding wizard
- Backwards compatibility documentation + semver policy

**Blocking:** None (independent work)
**Unblocks:** v3.1 (observability refinement), v3.2 (incident response)

### v3.1: Consumer Hardening II — Observability Maturity
**Target:** 1-2 weeks
**Scope:** ~8 issues
- Flywheel velocity dashboard improvements
- Structured hook logging expansion
- Cost tracking and budgeting
- Performance profiling on large repos

**Blocking:** v3.0 observability layer
**Unblocks:** v3.2 (incident response), v4.0 (resource awareness)

### v3.2: Consumer Hardening III — Incident Response
**Target:** 1-2 weeks
**Scope:** ~6 issues
- Incident runbook expansion
- Auto-recovery procedures
- Root-cause analysis helpers
- Consumer support playbooks

**Blocking:** v3.0, v3.1
**Unblocks:** v4.0 (multi-session autonomy)

### v3.3: Platform Expansion I — Cross-Vendor Consensus
**Target:** 2 weeks
**Scope:** ~10 issues
- Cross-vendor judge protocols
- Confidence weighting in verdicts
- `/council` vendor-neutral redesign
- Integration tests (Claude + Codex + N)

**Blocking:** None (parallel with v3.x)
**Unblocks:** v3.4, v3.5

### v3.4: Platform Expansion II — Orchestration Standardization
**Target:** 1-2 weeks
**Scope:** ~8 issues
- Swarm/Crank/Council mapping documentation
- Olympus bridge completion (constraint sync)
- Codex sub-agent execution reference
- Skill portability validation

**Blocking:** v3.3 (cross-vendor protocols)
**Unblocks:** v3.5, v4.0

### v3.5: Platform Expansion III — Flywheel Bridges
**Target:** 1-2 weeks
**Scope:** ~8 issues
- Learned-patterns export to Beads
- External knowledge injection interface
- Cross-project knowledge tagging
- Flywheel health metrics by external system

**Blocking:** v3.4 (orchestration stability)
**Unblocks:** v4.0 (learned system behavior)

### v3.6: Autonomous Scaling I — Multi-Session Bridging
**Target:** 2 weeks
**Scope:** ~10 issues
- Persistent embedding DB for learned state
- Session-to-session context injection
- Heuristic application from prior runs
- Integration tests (7+ day unattended runs)

**Blocking:** v3.5, v3.2 (observability)
**Unblocks:** v3.7

### v3.7: Autonomous Scaling II — Resource Awareness
**Target:** 1-2 weeks
**Scope:** ~8 issues
- Token budgeting per phase
- Wall-clock time awareness
- Model availability detection + fallback
- Backpressure handling (slow down when token-constrained)

**Blocking:** v3.6 (multi-session state)
**Unblocks:** v3.8

### v3.8: Autonomous Scaling III — Learned Workflows
**Target:** 2 weeks
**Scope:** ~8 issues
- Pattern mining from successful `/crank` runs
- Learned-template registration
- Skill generation from templates
- Cold-start project similarity matching

**Blocking:** v3.7 (resource awareness)
**Unblocks:** v4.0

---

## Three Themes Across All v3

Across all 8 v3 releases, three thematic threads run through every phase:

### Theme 1: Measurement as Discipline
**Hypothesis:** Systems that measure their behavior improve faster than those that don't.

In v2, GOALS.yaml proved this. Every v2.5+ release added fitness goals. By v2.7, the system diagnosed its own gaps (context windowing, pre-mortem enforcement, README identity) without human prompting.

v3 expands measurement:
- **Observability goals** — Track: flywheel velocity, goal fitness over time, phase timing distributions, error rates by skill, cost per run, tokens/minute trends
- **Consumer outcome metrics** — Track: first-success time, skill adoption curves, pre-mortem FAIL rate, vibe WARN rate, incident frequency
- **Platform metrics** — Track: cross-vendor agreement score, skill portability score, external knowledge retrieval accuracy
- **Autonomous system metrics** — Track: `/evolve` cycle time, fix quality (manual fix vs `/evolve` fix agreement), resource utilization

Every metric becomes a potential GOALS.yaml entry. System self-corrects when metrics drift.

### Theme 2: Boundaries as Features
**Hypothesis:** Clear constraints enable reliable autonomy.

v2.8 shipped `ao rpi phased` with per-phase context budgets. The constraint IS the feature — bounded phases are predictable and testable.

v3 expands boundaries:
- **Token budgets per phase** — Each phase knows its ceiling; can degrade gracefully
- **Wall-clock limits** — Session can time-box itself and resume later
- **Cost ceilings** — Stop when $X spent; ship partial work rather than overrun
- **Model constraints** — Fallback chain when preferred model unavailable
- **Worker safety limits** — Max workers per wave, max retries before escalation, max context per worker

Boundaries make the system safe for humans AND autonomous operation.

### Theme 3: Consumer-First Documentation
**Hypothesis:** Docs written for external users are simpler and more durable than internal docs.

v2.7.0 shipped the first "consumer README" — tagline, personas, 4-pillar framing. That README stayed stable (no rewrites in v2.8). Internal docs (CLAUDE.md, ARCHITECTURE.md) drift frequently.

v3 inverts the priority:
- **All new features documented for consumers first** — Before writing internal engineering docs
- **Every skill has consumer examples** — Not just reference docs
- **Backwards compatibility documented** — What won't change, what will change, migration paths
- **Multi-vendor guidance explicit** — "This works on Claude. On Codex use X instead."

This forces designs to be simple and orthogonal. Complex designs are hard to explain to consumers.

---

## README Freeze Recommendation

**Recommendation:** Freeze the README at v3.0.0 release. No rewrites until v4.0.

**Rationale:** v2 had 4 README rewrites (v2.0, v2.2, v2.4, v2.7), all driven by identity confusion or architectural pivots. By v2.7, identity was clear and the README was stable. v3 should maintain that stability.

**Implementation:**
- v3.0 README is the baseline for 3 releases (v3.0 through v3.2)
- v3.3-v3.8 releases update sections but don't rewrite narrative
- Any urge to rewrite = send to `/product` skill for customer input first
- v4.0 planning includes "should we rewrite the README?" decision

**Risk:** Consumer confusion if README drifts from reality → Mitigation: Each v3 release includes a "what changed since v3.0?" section in CHANGELOG that updates README pointers.

---

## Dependencies and Milestones

```
v3.0 ──────────> v3.1 ──────────> v3.2 ──────────────────┐
  │                                                       │
  └──> v3.3 ──────> v3.4 ──────────> v3.5 ──────────┐    │
                                                     ├──> v3.6 ──> v3.7 ──> v3.8
  (Arc 1 blocking Arc 3 at v3.6)                     │
                                                     └──────────────────────┘
```

**Critical Path:** v3.0 (consumer ready) → v3.1-3.2 (hardened) → v3.6-3.8 (autonomous scaling)

**Parallel Path:** v3.3-3.5 (platform expansion) can run alongside, but v3.5 must complete before v3.6 (flywheel bridges enable learned workflows)

---

## Success Metrics for v3

| Metric | Target | Measurement |
|--------|--------|-------------|
| **Consumer Adoption** | ≥3 external teams | GitHub org count in Beads |
| **Production Incidents** | <1 per week (v3.0-v3.2) | Incident runbook usage + feedback |
| **Multi-Runtime** | 4 vendors certified | Integration test matrix pass rate |
| **Cross-Vendor Agreement** | >80% in `/council` | Judge disagreement rate tracking |
| **Autonomous Run Duration** | 7+ days unattended | `/evolve` session logs, zero escalations |
| **Learned Fix Quality** | >80% of manual fixes | Manual audit of `/evolve` output vs human PRs |
| **Goal Fitness Trend** | Upward across v3 | GOALS.yaml CI trend (should improve, not degrade) |
| **README Stability** | Zero rewrites | Diff between v3.0, v3.2, v3.8 READMEs |

---

## Non-Goals for v3

What v3 is **not** trying to solve (deferred to v4+):

- **Custom agent generation** — "Write me an agent for X" skill (requires learned-workflow stabilization first)
- **Skill marketplace** — Public skill sharing and discovery (requires portability certification first)
- **Multi-model consensus at query time** — Running same question across 10 models (too expensive; focus on orchestration first)
- **Cross-project knowledge fusion** — Merging learnings across unrelated repos (requires learned state persistence first)
- **Rollback automation** — Auto-revert failed changes to production (requires incident runbook hardening first)

---

## Appendix: v2.x Comparative Timeline

For context, here's what v2.x actually achieved:

| Version | Date | Duration | Focus | Size |
|---------|------|----------|-------|------|
| **v2.0.0** | Feb 9 | - | RPI lifecycle, `/council`, `/swarm` | 1,250 lines CHANGELOG |
| **v2.1.0** | Feb 9 | <1 day | Public polish, install script, doctor | 960 lines |
| **v2.2.0** | Feb 10 | 1 day | Security fixes, standards, skill tiers, RPI gates | 1,550 lines (largest) |
| **v2.2.1** | Feb 10 | <1 day | `ao pool` commands, flywheel wiring | 280 lines |
| **v2.3.0** | Feb 11 | 1 day | Runtime-native backends, Codex sub-agents | 440 lines |
| **v2.4.0** | Feb 11 | <1 day | Knowledge flywheel integration, `ao forge` | 660 lines |
| **v2.5.0** | Feb 12 | 1 day | `/evolve` skill, `/product`, GOALS.yaml (11 goals) | 1,090 lines |
| **v2.5.1** | Feb 12 | <1 day | Skill validate.sh, RPI context windowing, incident runbook, GOALS.yaml (20 goals) | 1,200 lines |
| **v2.6.0** | Feb 13 | 1 day | Micro-epic fast path, examples + troubleshooting for all skills, `--test-first` | 850 lines |
| **v2.7.0** | Feb 13 | <1 day | `ao CLI improvements` (11 issues), README overhaul, GOALS.yaml (44 goals) | 1,080 lines |
| **v2.7.1** | Feb 14 | <1 day | `ao search` simplification, pre-mortem gate, integration tests | 380 lines |
| **v2.8.0** | Feb 15 | <1 day | `ao rpi phased`, 10 CLI wiring, 3-layer compaction tests, `.agents/` standardization | 1,200 lines |

**Total v2.x:** 12 releases in 6 days, 10,000+ lines of CHANGELOG.

**Insight:** After v2.3 (platform stabilization), releases focused on measurement + gates (v2.5+). The system became self-improving rather than manually directed. v3 scales that pattern from internal (system health) to external (consumer outcomes) and autonomous (learned behavior).

---

## Next Steps

1. **v3.0.0 planning** — Decompose consumer hardening arc into 12 issues with dependencies
2. **Measurement infrastructure setup** — Define GOALS.yaml entries for observability, consumer metrics, platform metrics
3. **Consumer research** — Interview 2-3 potential users: what's blocking adoption? What would make them trust AgentOps in production?
4. **Review this roadmap with deliberation council** — Before committing to v3.0, debate viability and ordering. Pre-mortem the plan.
